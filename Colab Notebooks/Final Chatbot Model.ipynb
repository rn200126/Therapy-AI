{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Chatbot Model.ipynb","provenance":[{"file_id":"1IAu3A74XUqVMXMZ7Z_DeW0UDhPoMUuDv","timestamp":1653156769745}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUpYsfh1NUhi","outputId":"c5d27c31-3dc9-4d0a-cd18-123a8e9bc645","executionInfo":{"status":"ok","timestamp":1653317186307,"user_tz":-120,"elapsed":9551,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Feature Extraction Libraries\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","\n","# Classifier Model libraries\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import tree\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.pipeline import Pipeline\n","\n","#Performance Metrics libraries\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","#Basic Libraries\n","import pandas as pd\n","import numpy as np\n","\n","#Text Libraries\n","import nltk \n","import string\n","import re\n","\n","#Visualization libraries\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#Other libraries\n","import pickle\n","import os\n","import random\n","from scipy.spatial import distance\n","from datetime import datetime\n","!pip install pendulum\n","!pip install nameparser\n","import pendulum\n","from nameparser.parser import HumanName\n","from nltk.corpus import wordnet\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pendulum in /usr/local/lib/python3.7/dist-packages (2.1.2)\n","Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.7/dist-packages (from pendulum) (2.8.2)\n","Requirement already satisfied: pytzdata>=2020.1 in /usr/local/lib/python3.7/dist-packages (from pendulum) (2020.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0,>=2.6->pendulum) (1.15.0)\n","Requirement already satisfied: nameparser in /usr/local/lib/python3.7/dist-packages (1.1.1)\n"]}]},{"cell_type":"code","source":["#Download packages\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tcxxmk8-OG8F","executionInfo":{"status":"ok","timestamp":1653317186756,"user_tz":-120,"elapsed":485,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}},"outputId":"e2d5520b-5536-488a-bd42-2ba17391b68f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#Rooth Path\n","#Drive Mount\n","root_path = \"/content/drive/MyDrive/FinalYearProject/Dataset/\"\n","resource_root_path = \"/content/drive/MyDrive/FinalYearProject/Dataset/\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOrQ-a-FG-Kp","executionInfo":{"status":"ok","timestamp":1653317188665,"user_tz":-120,"elapsed":1931,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}},"outputId":"9b1b013f-74f7-4e02-bb56-e5d3a46763b5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"AYTUGnfRN5k5","outputId":"d9edd826-cb34-4837-fe0f-e6b1c6a49f64","executionInfo":{"status":"ok","timestamp":1653317189137,"user_tz":-120,"elapsed":499,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Read Chatbot DataSet\n","df_chatbot = pd.read_csv(root_path+'Chatbot Dataset.csv',encoding='ISO-8859-1')\n","df_chatbot = df_chatbot.dropna(axis=0)\n","df_chatbot"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   User  \\\n","0                                                 Hello   \n","1                                                    Hi   \n","2                                                  Hola   \n","3                                              Hi there   \n","4                                             Hya there   \n","...                                                 ...   \n","2295  Today I meditated for 30 minutes and I feel am...   \n","2296                                    I broke my foot   \n","2297                                    I broke my foot   \n","2298  My boss gave me priase in front of the group a...   \n","2299  My boss gave me priase in front of the group a...   \n","\n","                                                Chatbot             Intent  \n","0                               Hi <HUMAN> how are you?           Greeting  \n","1                            Hello <HUMAN> how are you?           Greeting  \n","2                               Hi <HUMAN> how are you?           Greeting  \n","3                               Hi <HUMAN> how are you?           Greeting  \n","4                               Hi <HUMAN> how are you?           Greeting  \n","...                                                 ...                ...  \n","2295         I am glad you felt better after meditating    Surprise_Amazed  \n","2296                 I am sorry to hear your foot broke             Health  \n","2297             I am sorry to hear you broke your foot             Health  \n","2298                    I am glad your work was praised  Happy_Excited_Joy  \n","2299  I am happy to hear about the praise you got at...  Happy_Excited_Joy  \n","\n","[2064 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-ffb89625-c699-44e3-9caf-f092d15b0ec1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User</th>\n","      <th>Chatbot</th>\n","      <th>Intent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello</td>\n","      <td>Hi &lt;HUMAN&gt; how are you?</td>\n","      <td>Greeting</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi</td>\n","      <td>Hello &lt;HUMAN&gt; how are you?</td>\n","      <td>Greeting</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hola</td>\n","      <td>Hi &lt;HUMAN&gt; how are you?</td>\n","      <td>Greeting</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hi there</td>\n","      <td>Hi &lt;HUMAN&gt; how are you?</td>\n","      <td>Greeting</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hya there</td>\n","      <td>Hi &lt;HUMAN&gt; how are you?</td>\n","      <td>Greeting</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2295</th>\n","      <td>Today I meditated for 30 minutes and I feel am...</td>\n","      <td>I am glad you felt better after meditating</td>\n","      <td>Surprise_Amazed</td>\n","    </tr>\n","    <tr>\n","      <th>2296</th>\n","      <td>I broke my foot</td>\n","      <td>I am sorry to hear your foot broke</td>\n","      <td>Health</td>\n","    </tr>\n","    <tr>\n","      <th>2297</th>\n","      <td>I broke my foot</td>\n","      <td>I am sorry to hear you broke your foot</td>\n","      <td>Health</td>\n","    </tr>\n","    <tr>\n","      <th>2298</th>\n","      <td>My boss gave me priase in front of the group a...</td>\n","      <td>I am glad your work was praised</td>\n","      <td>Happy_Excited_Joy</td>\n","    </tr>\n","    <tr>\n","      <th>2299</th>\n","      <td>My boss gave me priase in front of the group a...</td>\n","      <td>I am happy to hear about the praise you got at...</td>\n","      <td>Happy_Excited_Joy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2064 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffb89625-c699-44e3-9caf-f092d15b0ec1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ffb89625-c699-44e3-9caf-f092d15b0ec1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ffb89625-c699-44e3-9caf-f092d15b0ec1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"yqTSh14pNzf7","outputId":"fe16170d-96d4-4b52-e991-75928ce8ef90","executionInfo":{"status":"ok","timestamp":1653317189143,"user_tz":-120,"elapsed":56,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Read Emotions Dataset\n","df_emotion = pd.read_csv(root_path+'text_emotions_neutral.csv')\n","df_emotion"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 content sentiment\n","0                                i didnt feel humiliated   sadness\n","1      i can go from feeling so hopeless to so damned...   sadness\n","2       im grabbing a minute to post i feel greedy wrong     anger\n","3      i am ever feeling nostalgic about the fireplac...      love\n","4                                   i am feeling grouchy     anger\n","...                                                  ...       ...\n","24995   Yeah.  Did you know that in Nevada there is a...   Neutral\n","24996   I wonder why, not many have had facial hair a...   Neutral\n","24997   That is sad, it is bad that we really wind up...   Neutral\n","24998   Same here.  Since 1900 the taller candidate h...   Neutral\n","24999   I do, I even read a book about their developm...   Neutral\n","\n","[25000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-2f540a7d-2e7b-489d-896e-a7fb4a471e9b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i didnt feel humiliated</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i can go from feeling so hopeless to so damned...</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>im grabbing a minute to post i feel greedy wrong</td>\n","      <td>anger</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i am ever feeling nostalgic about the fireplac...</td>\n","      <td>love</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am feeling grouchy</td>\n","      <td>anger</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>Yeah.  Did you know that in Nevada there is a...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>I wonder why, not many have had facial hair a...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>That is sad, it is bad that we really wind up...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>Same here.  Since 1900 the taller candidate h...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>I do, I even read a book about their developm...</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f540a7d-2e7b-489d-896e-a7fb4a471e9b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2f540a7d-2e7b-489d-896e-a7fb4a471e9b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2f540a7d-2e7b-489d-896e-a7fb4a471e9b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"1Q_AV5W8O7Ni"},"source":["# 1. Pre-processing of the data"]},{"cell_type":"code","metadata":{"id":"QX2OZdAmO6i5","executionInfo":{"status":"ok","timestamp":1653317189147,"user_tz":-120,"elapsed":49,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Pre-process all the textual data based on different techniques\n","class Preprocessing:\n","    #Remove stopwords, remove and replace emojis with corresponding text\n","    def __init__(self,Remove_stopwords=True):\n","        self.emojis = pd.read_csv(resource_root_path+'emojis.txt',sep=',',header=None)\n","        self.emojis_dict = {i:j for i,j in zip(self.emojis[0],self.emojis[1])}\n","        self.pattern = '|'.join(sorted(re.escape(k) for k in self.emojis_dict))\n","        nltk.download('stopwords')\n","        nltk.download('wordnet')\n","        self.rmv_stopword = Remove_stopwords\n","    #Replace emojis\n","    def replace_emojis(self, text):\n","        text = re.sub(self.pattern,lambda m: self.emojis_dict.get(m.group(0)), text, flags=re.IGNORECASE)\n","        return text\n","    #Remove punctuation\n","    def remove_punctuation(self, text):\n","        text = self.replace_emojis(text)\n","        text  = \"\".join([char for char in text if char not in string.punctuation])\n","        text = re.sub('[0-9]+', '', text)\n","        return text\n","    #Tokenise text\n","    def tokenization(self, text):\n","        text = text.lower()\n","        text = re.split('\\W+', text)\n","        return text\n","    #Remove stopwords \n","    def remove_stopwords(self, text):\n","        stopword = nltk.corpus.stopwords.words('english')\n","       # stopword.extend(['yr', 'year', 'woman', 'man', 'girl','boy','one', 'two', 'sixteen', 'yearold', 'fu', 'weeks', 'week',\n","           #   'treatment', 'associated', 'patients', 'may','day', 'case','old','u','n','didnt','ive','ate','feel','keep'\n","           #     ,'brother','dad','basic','im',''])        \n","        text = [word for word in text if word not in stopword]\n","        return text\n","    #Lemmatise words \n","    def lemmatizer(self, text):\n","        wn = nltk.WordNetLemmatizer()\n","        text = [wn.lemmatize(word) for word in text]\n","        return text\n","    #Return clean textual data thanks to the previous implemented techniques\n","    def clean_text(self, text):\n","        text = self.remove_punctuation(text)\n","        text = self.tokenization(text)\n","        if self.rmv_stopword == True:\n","            text = self.remove_stopwords(text)\n","        text = self.lemmatizer(text)\n","        return text"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8L5on6JQR_w1","outputId":"de1a9670-4b2c-49ef-d2d8-615e3ef6ae68","executionInfo":{"status":"ok","timestamp":1653317191407,"user_tz":-120,"elapsed":2306,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["preprocess = Preprocessing(Remove_stopwords=False)\n","\n","df_test = df_chatbot['User'].apply(lambda x: preprocess.clean_text(x))\n","df_test"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["0                                                 [hello]\n","1                                                    [hi]\n","2                                                  [hola]\n","3                                             [hi, there]\n","4                                            [hya, there]\n","                              ...                        \n","2295    [today, i, meditated, for, minute, and, i, fee...\n","2296                                 [i, broke, my, foot]\n","2297                                 [i, broke, my, foot]\n","2298    [my, bos, gave, me, priase, in, front, of, the...\n","2299    [my, bos, gave, me, priase, in, front, of, the...\n","Name: User, Length: 2064, dtype: object"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"p0d00Y9cQ_at"},"source":["# 3. Feature Extraction"]},{"cell_type":"code","metadata":{"id":"Jiy9VbnzTvVh","executionInfo":{"status":"ok","timestamp":1653317191413,"user_tz":-120,"elapsed":90,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Perform Feature Extraction\n","class FeatureExtraction:\n","    def __init__(self,rmv_stopword=True):\n","        self.rmv_stopword = rmv_stopword\n","        self.preprocess = Preprocessing(self.rmv_stopword)\n","        self.countVectorizer1 = CountVectorizer(analyzer=self.preprocess.clean_text)\n","        self.tfidf_transformer_xtrain = TfidfTransformer()\n","        self.tfidf_transformer_xtest = TfidfTransformer()\n","    #Get features from the training and testing data\n","    def get_features(self, X_train, X_test):\n","        countVector1 = self.countVectorizer1.fit_transform(X_train)\n","        countVector2 = self.countVectorizer1.transform(X_test)\n","\n","        x_train = self.tfidf_transformer_xtrain.fit_transform(countVector1)\n","        x_test = self.tfidf_transformer_xtest.fit_transform(countVector2)\n","\n","        return x_train, x_test\n","\n","    def get_processed_text(self, input_str):\n","        return self.tfidf_transformer_xtest.fit_transform(self.countVectorizer1.transform([input_str]))\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ad3qGEn8Uvok","outputId":"ab544b67-1683-49f0-8acc-89557c15e752","executionInfo":{"status":"ok","timestamp":1653317205670,"user_tz":-120,"elapsed":14333,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Split text and train data and then apply Feature Extraction on top. \n","X_train_ed, X_test_ed, y_train_ed, y_test_ed = train_test_split(df_emotion['content'], df_emotion['sentiment'],test_size=0.3, random_state = 116)\n","X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(df_chatbot['User'], df_chatbot['Intent'],test_size=0.25, random_state = 16)\n","\n","fe_cb = FeatureExtraction(rmv_stopword=False)\n","fe_ed = FeatureExtraction(rmv_stopword=True)\n","\n","x_train_ed, x_test_ed = fe_ed.get_features(X_train_ed, X_test_ed)\n","x_train_cb, x_test_cb = fe_cb.get_features(X_train_cb, X_test_cb)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"1rj-H4aqVuB3"},"source":["# 4. Models\n"," "]},{"cell_type":"code","metadata":{"id":"syXVrXsGVywC","executionInfo":{"status":"ok","timestamp":1653317206601,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["class Models:\n","    def __init__(self, X_train, Y_train, X_test, Y_test, model_name='cb'):\n","        self.x_train = X_train\n","        self.x_test = X_test\n","        self.y_test = Y_test\n","        self.y_train = Y_train\n","        self.chatbot_model_file = root_path+'ChatbotModels.pkl'\n","        self.emotion_model_file = root_path+'EmotionDetectionModels.pkl'  \n","\n","        self.chatbot_summary_file = root_path+'ChatbotModelsSummary.pkl'\n","        self.emotion_summary_file = root_path+'EmotionDetectionModelsSummary.pkl' \n","        self.model_name = model_name  \n","\n","        self.svm = SGDClassifier()\n","        self.logisticRegr = LogisticRegression()\n","        self.rfc = RandomForestClassifier(n_estimators=1, random_state=0)\n","        self.mnb = MultinomialNB()\n","        self.dt = tree.DecisionTreeClassifier()\n","        self.mlp = MLPClassifier(random_state=5, max_iter=300)\n","\n","        self.svm_summary = {}\n","        self.lr_summary = {}\n","        self.rfc_summary = {}\n","        self.mnb_summary = {}\n","        self.dt_summary = {}\n","        self.mlp_summary = {}\n","\n","    def load_models(self):\n","        if self.model_name == 'ed':\n","            if os.path.isfile(self.emotion_model_file):\n","                with open(self.emotion_model_file,'rb') as f:\n","                    self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp = pickle.load(f)\n","\n","                with open(self.emotion_summary_file,'rb') as f:\n","                    self.svm_summary, self.lr_summary, self.rfc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary = pickle.load(f)\n","                    print('Emotion Detection Models retrived from Disk successfully')\n","                    return self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp\n","            else:\n","                self.train_models()\n","                self.save_models()\n","                return self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp\n","        elif self.model_name == 'cb':\n","            if os.path.isfile(self.chatbot_model_file):  \n","                with open(self.chatbot_model_file,'rb') as f:\n","                    self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp = pickle.load(f)\n","\n","                with open(self.chatbot_summary_file,'rb') as f:\n","                    self.svm_summary, self.lr_summary, self.rfc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary = pickle.load(f)\n","                    print('Chabot Models retrived from Disk successfully')\n","                    return self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp\n","            else:\n","                self.train_models()\n","                self.save_models()\n","                return self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp\n","\n","    def train_models(self):\n","        print('-----Model Training-----')\n","        print('Training SVM...')\n","        self.SVM()\n","        print('Training Logistic Regression...')\n","        self.LR()\n","        print('Training Random Forest...')\n","        self.RFC()\n","        print('Training Multinomial Naive Bayes...')\n","        self.MNB()\n","        print('Training Decision Tree...')\n","        self.DT()\n","        print('Training Multi-Layer Perceptron Model...')\n","        self.MLP()\n","        print('Successfully Trained All Models')\n","\n","        return self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp\n","          \n","\n","    def SVM(self):\n","        self.svm.fit(self.x_train, self.y_train)\n","        y_pred = self.svm.predict(self.x_test)\n","\n","        svm_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        svm_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        svm_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        svm_cm = confusion_matrix(self.y_test,y_pred)\n","        svm_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.svm_summary['Accuracy'] = svm_acc\n","        self.svm_summary['Precision'] = svm_prec\n","        self.svm_summary['Recall'] = svm_recal\n","        self.svm_summary['F1'] = svm_f1\n","        self.svm_summary['CM'] = svm_cm\n","    \n","    def LR(self):\n","        self.logisticRegr.fit(self.x_train, self.y_train)\n","\n","        y_pred = self.logisticRegr.predict(self.x_test)\n","\n","        lr_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        lr_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        lr_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        lr_cm = confusion_matrix(self.y_test,y_pred)\n","        lr_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.lr_summary['Accuracy'] = lr_acc\n","        self.lr_summary['Precision'] = lr_prec\n","        self.lr_summary['Recall'] = lr_recal\n","        self.lr_summary['F1'] = lr_f1\n","        self.lr_summary['CM'] = lr_cm\n","\n","    def RFC(self):\n","        self.rfc.fit(self.x_train, self.y_train)\n","\n","        y_pred = self.rfc.predict(self.x_test)\n","\n","        rfc_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        rfc_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        rfc_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        rfc_cm = confusion_matrix(self.y_test,y_pred)\n","        rfc_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.rfc_summary['Accuracy'] = rfc_acc\n","        self.rfc_summary['Precision'] = rfc_prec\n","        self.rfc_summary['Recall'] = rfc_recal\n","        self.rfc_summary['F1'] = rfc_f1\n","        self.rfc_summary['CM'] = rfc_cm\n","\n","\n","    def MNB(self):\n","        self.mnb.fit(self.x_train, self.y_train)\n","\n","        y_pred = self.mnb.predict(self.x_test)\n","\n","        mnb_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        mnb_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        mnb_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        mnb_cm = confusion_matrix(self.y_test,y_pred)\n","        mnb_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.mnb_summary['Accuracy'] = mnb_acc\n","        self.mnb_summary['Precision'] = mnb_prec\n","        self.mnb_summary['Recall'] = mnb_recal\n","        self.mnb_summary['F1'] = mnb_f1\n","        self.mnb_summary['CM'] = mnb_cm\n","\n","    def DT(self):\n","        self.dt.fit(self.x_train, self.y_train)\n","        y_pred = self.dt.predict(self.x_test)\n","\n","        dt_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        dt_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        dt_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        dt_cm = confusion_matrix(self.y_test,y_pred)\n","        dt_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.dt_summary['Accuracy'] = dt_acc\n","        self.dt_summary['Precision'] = dt_prec\n","        self.dt_summary['Recall'] = dt_recal\n","        self.dt_summary['F1'] = dt_f1\n","        self.dt_summary['CM'] = dt_cm\n","\n","    def MLP(self):\n","        self.mlp.fit(self.x_train, self.y_train)\n","        y_pred = self.mlp.predict(self.x_test)\n","\n","        mlp_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n","        mlp_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n","        mlp_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n","        mlp_cm = confusion_matrix(self.y_test,y_pred)\n","        mlp_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n","        self.mlp_summary['Accuracy'] = mlp_acc\n","        self.mlp_summary['Precision'] = mlp_prec\n","        self.mlp_summary['Recall'] = mlp_recal\n","        self.mlp_summary['F1'] = mlp_f1\n","        self.mlp_summary['CM'] = mlp_cm\n","\n","    def model_summary(self):\n","        return self.svm_summary, self.lr_summary, self.rfc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary\n","\n","    def save_models(self):\n","      if self.model_name == 'ed':\n","          with open(self.emotion_model_file, 'wb') as f:\n","              pickle.dump([self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp], f)\n","\n","          with open(self.emotion_summary_file, 'wb') as f:\n","              pickle.dump([self.svm_summary, self.lr_summary, self.rfc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary], f)\n","\n","          print('Emotion Detection Models saved successfully in the disk')\n","      elif self.model_name == 'cb':\n","          with open(self.chatbot_model_file, 'wb') as f:\n","              pickle.dump([self.svm, self.logisticRegr, self.rfc, self.mnb, self.dt, self.mlp], f)\n","\n","          with open(self.chatbot_summary_file, 'wb') as f:\n","              pickle.dump([self.svm_summary, self.lr_summary, self.rfc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary], f)\n","\n","          print('Chatbot Models saved successfully in the disk')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjN-CPxiurd-","executionInfo":{"status":"ok","timestamp":1653317206608,"user_tz":-120,"elapsed":44,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["chatbot_models = Models(x_train_cb, y_train_cb, x_test_cb, y_test_cb, model_name='cb')\n","emotion_models = Models(x_train_ed, y_train_ed, x_test_ed, y_test_ed, model_name='ed')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bd3eQPGkykuL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bc04b1b-ed80-436d-cc24-d64d708c174b","executionInfo":{"status":"ok","timestamp":1653317646194,"user_tz":-120,"elapsed":439620,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Load all the models\n","#Chatbot Models\n","svm_cb, logisticRegr_cb, rfc_cb, mnb_cb, dt_cb, mlp_cb = chatbot_models.load_models()\n","svm_summary_cb, lr_summary_cb, rfc_summary_cb, mnb_summary_cb, dt_summary_cb, mlp_summary_cb = chatbot_models.model_summary()\n","#Emotions Models\n","svm_ed, logisticRegr_ed, rfc_ed, mnb_ed, dt_ed, mlp_ed = emotion_models.load_models()\n","svm_summary_ed, lr_summary_ed, rfc_summary_ed, mnb_summary_ed, dt_summary_ed, mlp_summary_ed = emotion_models.model_summary()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["-----Model Training-----\n","Training SVM...\n","Training Logistic Regression...\n","Training Random Forest...\n","Training Multinomial Naive Bayes...\n","Training Decision Tree...\n","Training Multi-Layer Perceptron Model...\n","Successfully Trained All Models\n","Chatbot Models saved successfully in the disk\n","-----Model Training-----\n","Training SVM...\n","Training Logistic Regression...\n","Training Random Forest...\n","Training Multinomial Naive Bayes...\n","Training Decision Tree...\n","Training Multi-Layer Perceptron Model...\n","Successfully Trained All Models\n","Emotion Detection Models saved successfully in the disk\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czqB-sBo-Kos","outputId":"57b37319-3acf-4512-bec9-8776def85584","executionInfo":{"status":"ok","timestamp":1653317661816,"user_tz":-120,"elapsed":15719,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Train the Chatbot models\n","svm_cb, logisticRegr_cb, rfc_cb, mnb_cb, dt_cb, mlp_cb = chatbot_models.train_models()\n","svm_summary_cb, lr_summary_cb, rfc_summary_cb, mnb_summary_cb, dt_summary_cb, mlp_summary_cb = chatbot_models.model_summary()\n","chatbot_models.save_models()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["-----Model Training-----\n","Training SVM...\n","Training Logistic Regression...\n","Training Random Forest...\n","Training Multinomial Naive Bayes...\n","Training Decision Tree...\n","Training Multi-Layer Perceptron Model...\n","Successfully Trained All Models\n","Chatbot Models saved successfully in the disk\n"]}]},{"cell_type":"code","source":["#Train the Emotion Models\n","svm_ed, logisticRegr_ed, rfc_ed, mnb_ed, dt_ed, mlp_ed = emotion_models.train_models()\n","svm_summary_ed, lr_summary_ed, rfc_summary_ed,mnb_summary_ed, dt_summary_ed, mlp_summary_ed = emotion_models.model_summary()\n","emotion_models.save_models()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUtat35sQbAi","executionInfo":{"status":"ok","timestamp":1653318067139,"user_tz":-120,"elapsed":405414,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}},"outputId":"a50069c8-d4cd-41b4-ab36-63335bebef7e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["-----Model Training-----\n","Training SVM...\n","Training Logistic Regression...\n","Training Random Forest...\n","Training Multinomial Naive Bayes...\n","Training Decision Tree...\n","Training Multi-Layer Perceptron Model...\n","Successfully Trained All Models\n","Emotion Detection Models saved successfully in the disk\n"]}]},{"cell_type":"markdown","metadata":{"id":"iuGzMxw1XSqZ"},"source":["# 5. Chatbot"]},{"cell_type":"code","metadata":{"id":"G9xU8nnMaDjY","executionInfo":{"status":"ok","timestamp":1653318067646,"user_tz":-120,"elapsed":535,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["class Chatbot:\n","    def __init__(self):\n","        accuracies = np.array([svm_summary_cb['Accuracy'], lr_summary_cb['Accuracy'], rfc_summary_cb['Accuracy'],  \n","             mnb_summary_cb['Accuracy'], dt_summary_cb['Accuracy'], mlp_summary_cb['Accuracy']])\n","        norm_accuracy = accuracies - min(accuracies)\n","        self.model_weight = norm_accuracy/sum(norm_accuracy)\n","        self.Intents = df_chatbot['Intent'].unique()\n","        self.Human_name = 'Hridoy'\n","    #Generates replies of the chatbot\n","    def generate_replies(self, text, intent_name):\n","        reply = self.respond(text, intent_name)\n","        return reply\n","    #Calculate Cosine Distance\n","    def cosine_distance_countvectorizer(self, s1, s2):    \n","        allsentences = [s1 , s2]\n","\n","        vectorizer = CountVectorizer()\n","        all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n","\n","        text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n","        text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n","\n","        cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n","        return round((1-cosine),2)\n","    #Give reply based on intent\n","    def reply(self, text, intent_name):\n","        maximum = float('-inf')\n","        response = \"\"\n","        closest = \"\"\n","        replies = {}\n","        list_sim, list_replies = [],[]\n","        dataset = df_chatbot[df_chatbot['Intent']==intent_name]\n","        for i in dataset.iterrows():\n","            sim = self.cosine_distance_countvectorizer_method(text, i[1]['User'])\n","            list_sim.append(sim)\n","            list_replies.append(i[1]['Chatbot'])\n","\n","        for i in range(len(list_sim)):\n","            if list_sim[i] in replies:\n","                replies[list_sim[i]].append(list_replies[i])\n","            else:\n","                replies[list_sim[i]] = list()\n","                replies[list_sim[i]].append(list_replies[i])\n","        d1 = sorted(replies.items(), key = lambda pair:pair[0],reverse=True)\n","        return d1[0][1][random.randint(0,len(d1[0][1])-1)]\n","\n","\n","    def extract_best_intent(self, list_intent_pred):\n","        intent_scores = {}\n","        for intent in self.Intents:\n","            intent_scores[intent] = 0.0   \n","        for i in range(len(list_intent_pred)):\n","            intent_scores[list_intent_pred[i]] += self.model_weight[i]\n","        si = sorted(intent_scores.items(), key = lambda pair:pair[1],reverse=True)[:6]\n","        return si[0][0], round(si[0][1],2)\n","\n","    def get_human_names(self, text):\n","        person_list = []\n","        person_names=person_list\n","        tokens = nltk.tokenize.word_tokenize(text)\n","        pos = nltk.pos_tag(tokens)\n","        sentt = nltk.ne_chunk(pos, binary = False)\n","\n","        person = []\n","        name = \"\"\n","        for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n","            for leaf in subtree.leaves():\n","                person.append(leaf[0])\n","            if len(person) > 0:  \n","                for part in person:\n","                    name += part + ' '\n","                if name[:-1] not in person_list:\n","                    person_list.append(name[:-1])\n","                name = ''\n","            person = []\n","        return person_list\n","\n","    def replace_tag(self, text):\n","        text = text.replace('<HUMAN>',self.Human_name)\n","\n","        # get current time\n","        BDT = pendulum.timezone('Europe/London')\n","        cdt = datetime.timetuple(datetime.now(BDT))\n","        hrs = int(cdt[3])\n","        am_pm = 'am'\n","        if int(cdt[3]) > 12:\n","            hrs = int(cdt[3]) - 12\n","            am_pm = 'pm'\n","\n","        current_time = str(cdt[2])+'-'+str(cdt[1])+'-'+str(cdt[0]) + ' '+ str(hrs)+':'+str(cdt[4])+' '+am_pm\n","        text = text.replace('<TIME>',current_time)\n","        return text\n","\n","    def chatbot_reply(self, text):\n","        processed_text = fe_cb.get_processed_text(text)\n","\n","        if self.get_human_names(text):\n","            self.Human_name = self.get_human_names(text)[0]\n","\n","        print('Intent using SVM: ',end = '')\n","        svm_intent = svm_cb.predict(processed_text)[0]\n","        lr_intent = logisticRegr_cb.predict(processed_text)[0]\n","        dt_intent = dt_cb.predict(processed_text)[0]\n","        mnb_intent = mnb_cb.predict(processed_text)[0]\n","        rfc_intent = rfc_cb.predict(processed_text)[0]\n","        mlp_intent = mlp_cb.predict(processed_text)[0]\n","        print(svm_intent)\n","        \n","        print('Intent using Logistic Regression: ',end = '')\n","        print(lr_intent)\n","        print('Intent using Decision Tree: ',end = '')\n","        print(dt_intent)\n","        print('Intent using Naive Bayes: ',end = '')\n","        print(mnb_intent)\n","        print('Intent using Random Forest: ',end = '')\n","        print(rfc_intent)\n","        print('Intent using Multi-Layer Perceptron: ',end = '')\n","        print(mlp_intent)\n","\n","\n","        #Generate a reply\n","        list_intent = [svm_intent, lr_intent, rfc_intent, mnb_intent, dt_intent, mlp_intent]\n","        best_intent, prob = self.extract_best_intent(list_intent)\n","        print('Best Intent:',best_intent,':',prob)\n","\n","        reply = \"I'm not sure what you mean...\" if prob < 0.4 else self.response_generate(text, best_intent)\n","\n","        reply = self.replace_tag(reply)\n","        print('Bot:',reply)\n","        print()\n","        return reply, prob, best_intent"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcA4moDWdn7x","executionInfo":{"status":"ok","timestamp":1653318067654,"user_tz":-120,"elapsed":94,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["class Emotion:\n","    def __init__(self):\n","        self.Emotions = df_emotion['sentiment'].unique()\n","        accuracies = np.array([svm_summary_ed['Accuracy'], lr_summary_ed['Accuracy'], rfc_summary_ed['Accuracy'],\n","             mnb_summary_ed['Accuracy'], dt_summary_ed['Accuracy'], mlp_summary_ed['Accuracy']])\n","        norm_accuracy = accuracies - min(accuracies)\n","        self.emotion_model_weight = norm_accuracy/sum(norm_accuracy)\n","\n","\n","    def extract_emotions(self, list_emotion_prediction):\n","        emotion_scores = {}\n","        for emotions in self.Emotions:\n","            emotion_scores[emotions] = 0.0   \n","        for i in range(len(list_emotion_prediction)):\n","            emotion_scores[list_emotion_prediction[i]] += self.emotion_model_weight[i]\n","        se = sorted(emotion_scores.items(), key = lambda pair:pair[1],reverse=True)\n","        return se[0][0], round(se[0][1],2)\n","\n","    def detect_emotion(self, text):\n","        processed_text = fe_ed.get_processed_text(text)\n","\n","        svm_emotion = svm_ed.predict(processed_text)[0]\n","        lr_emotion = logisticRegr_ed.predict(processed_text)[0]\n","        dt_emotion = dt_ed.predict(processed_text)[0]\n","        mnb_emotion = mnb_ed.predict(processed_text)[0]\n","        rfc_emotion = rfc_ed.predict(processed_text)[0]\n","        mlp_emotion = mlp_ed.predict(processed_text)[0]\n","\n","        list_emotion_pred = [svm_emotion, lr_emotion, rfc_emotion, mnb_emotion, dt_emotion, mlp_emotion]\n","        best_emotion, prob = self.extract_best_emotion(list_emotion_pred)\n","        print('Best Emotion:',best_emotion,':',prob)\n","\n","        print('Emotion using SVM: ',end = '')\n","        print(svm_emotion)\n","        print('Emotion using Logistic Regression: ',end = '')\n","        print(lr_emotion)\n","        print('Emotion using Decision Tree: ',end = '')\n","        print(dt_emotion)\n","        print('Emotion using Naive Bayes: ',end = '')\n","        print(mnb_emotion)\n","        print('Emotion using Random Forest: ',end = '')\n","        print(rfc_emotion)\n","        print('Emotion using Multi-Layer Perceptron ',end = '')\n","        print(mlp_emotion)\n","        print()\n","        return best_emotion, prob"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAVSuu-o3wkM","executionInfo":{"status":"ok","timestamp":1653318067659,"user_tz":-120,"elapsed":95,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Initialise instances of Chatbot and Emotions\n","chatbot = Chatbot()\n","emotion = Emotion()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bq90NxajUs-8","outputId":"5c0a347d-0047-424c-9411-e2b0afbc4103","executionInfo":{"status":"ok","timestamp":1653318067662,"user_tz":-120,"elapsed":89,"user":{"displayName":"Rashnah Nanthakumar","userId":"02056650333526485345"}}},"source":["#Print the accuracy of the models\n","accuracies = np.array([svm_summary_ed['Accuracy'], lr_summary_ed['Accuracy'], rfc_summary_ed['Accuracy'], \n","             mnb_summary_ed['Accuracy'], dt_summary_ed['Accuracy'], mlp_summary_ed['Accuracy']])\n","norm_accuracy = accuracies - min(accuracies)\n","chatbot_model_weight = norm_accuracy/sum(norm_accuracy)\n","\n","\n","print('SVM:',svm_summary_ed['Accuracy'])\n","print('Logistic regression:',lr_summary_ed['Accuracy'])\n","print('Random forest:',rfc_summary_ed['Accuracy'])\n","print('Naive Bayes:',mnb_summary_ed['Accuracy'])\n","print('Decision Tree:',dt_summary_ed['Accuracy'])\n","print('MLP:',mlp_summary_ed['Accuracy'])\n","chatbot_model_weight"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM: 91.493\n","Logistic regression: 89.56\n","Random forest: 67.96\n","Naive Bayes: 73.227\n","Decision Tree: 89.8\n","MLP: 88.76\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0.25293422, 0.23215821, 0.        , 0.05661006, 0.23473775,\n","       0.22355976])"]},"metadata":{},"execution_count":18}]}]}